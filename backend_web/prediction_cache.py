"""
ì‚¬ìš©ìž-ìŒì‹ì  ì˜ˆì¸¡ ì ìˆ˜ ìºì‹± ì„œë¹„ìŠ¤
"""

import logging
import httpx
import os
import asyncio
from datetime import datetime, timezone
from sqlalchemy.orm import Session
from sqlalchemy import and_

import models

logger = logging.getLogger(__name__)

# AI ëª¨ë¸ ì„œë²„ URL
MODEL_SERVER_URL = os.getenv("MODEL_API_URL", "https://backendmodel-production-77a7.up.railway.app")

# ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •
CONCURRENCY = int(os.getenv("PREDICTION_CONCURRENCY", "1"))
CHUNK_SIZE = int(os.getenv("PREDICTION_CHUNK_SIZE", "50"))
TIMEOUT = int(os.getenv("PREDICTION_TIMEOUT", "360"))


async def predict_for_business(
    client: httpx.AsyncClient,
    semaphore: asyncio.Semaphore,
    user_data: dict,
    business: "models.Business",
    calculated_at: datetime
) -> dict:
    """
    ë‹¨ì¼ ìŒì‹ì ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ìˆ˜í–‰ (ë³‘ë ¬ ì‹¤í–‰ìš© í—¬í¼ í•¨ìˆ˜)
    
    Args:
        client: HTTP í´ë¼ì´ì–¸íŠ¸
        semaphore: ë™ì‹œ ì‹¤í–‰ ì œì–´ìš© ì„¸ë§ˆí¬ì–´
        user_data: ì‚¬ìš©ìž ë°ì´í„° dict
        business: ìŒì‹ì  ê°ì²´
        calculated_at: ê³„ì‚° ì‹œê°
    
    Returns:
        dict: {
            "success": bool,
            "business_id": int,
            "business_name": str,
            "deepfm_score": float,
            "multitower_score": float,
            "request_time": float,
            "error": str (ì‹¤íŒ¨ ì‹œ)
        }
    """
    import time
    
    async with semaphore:
        try:
            start_time = time.time()
            
            response = await client.post(
                f"{MODEL_SERVER_URL}/predict_rating",
                json={
                    "user_data": user_data,
                    "business_data": {
                        "stars": business.stars,
                        "review_count": business.review_count,
                        "latitude": business.latitude,
                        "longitude": business.longitude,
                        "absa_features": business.absa_features or {}
                    }
                }
            )
            
            request_time = time.time() - start_time
            
            if response.status_code == 200:
                data = response.json()
                return {
                    "success": True,
                    "business_id": business.id,
                    "business_name": business.name or f"Business {business.id}",
                    "deepfm_score": data.get("deepfm_rating", 3.0),
                    "multitower_score": data.get("multitower_rating", 3.0),
                    "request_time": request_time
                }
            else:
                return {
                    "success": False,
                    "business_id": business.id,
                    "business_name": business.name or f"Business {business.id}",
                    "error": f"HTTP {response.status_code}",
                    "request_time": request_time
                }
                
        except httpx.TimeoutException:
            request_time = time.time() - start_time
            return {
                "success": False,
                "business_id": business.id,
                "business_name": business.name or f"Business {business.id}",
                "error": "Timeout",
                "request_time": request_time
            }
            
        except Exception as e:
            request_time = time.time() - start_time
            return {
                "success": False,
                "business_id": business.id,
                "business_name": business.name or f"Business {business.id}",
                "error": str(e),
                "request_time": request_time
            }


async def calculate_and_store_predictions(user_id: int, db: Session):
    """
    íŠ¹ì • ì‚¬ìš©ìžì˜ ëª¨ë“  ìŒì‹ì ì— ëŒ€í•œ ì˜ˆì¸¡ ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ê³  DBì— ì €ìž¥ (ì²­í¬ ê¸°ë°˜ ë³‘ë ¬ ì²˜ë¦¬)
    
    Args:
        user_id: ì‚¬ìš©ìž ID
        db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
    """
    import time
    
    total_start_time = time.time()
    logger.info(f"ðŸ”® [Prediction Cache] ì‚¬ìš©ìž {user_id}ì˜ ì˜ˆì¸¡ ê³„ì‚° ì‹œìž‘")
    
    try:
        # 1. ì‚¬ìš©ìž ì •ë³´ ì¡°íšŒ
        user = db.query(models.User).filter(models.User.id == user_id).first()
        if not user:
            logger.error(f"âŒ [Prediction Cache] ì‚¬ìš©ìž {user_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return
        
        # 2. ëª¨ë“  ìŒì‹ì  ì¡°íšŒ
        businesses = db.query(models.Business).all()
        total_businesses = len(businesses)
        total_chunks = (total_businesses - 1) // CHUNK_SIZE + 1
        
        logger.info(f"ðŸ“Š [Prediction Cache] {total_businesses}ê°œ ìŒì‹ì ì— ëŒ€í•´ ì˜ˆì¸¡ ê³„ì‚° ì¤‘")
        logger.info(f"   âš™ï¸  ì„¤ì •: concurrency={CONCURRENCY}, chunk={CHUNK_SIZE}, timeout={TIMEOUT}s")
        
        # 3. ì‚¬ìš©ìž ë°ì´í„° ë¯¸ë¦¬ ì¤€ë¹„ (ë°˜ë³µ ì‚¬ìš©)
        user_data = {
            "review_count": user.review_count,
            "useful": user.useful,
            "compliment": user.compliment,
            "fans": user.fans,
            "average_stars": user.average_stars,
            "yelping_since_days": user.yelping_since_days,
            "absa_features": user.absa_features or {}
        }
        
        # 4. ì „ì—­ í†µê³„ ë³€ìˆ˜
        calculated_at = datetime.now(timezone.utc)
        success_count = 0
        error_count = 0
        timeout_count = 0
        api_call_times = []
        slow_businesses = []  # 3ì´ˆ ì´ìƒ ê±¸ë¦° ìš”ì²­ë“¤
        
        # 5. Semaphore ìƒì„± (ë™ì‹œ ì‹¤í–‰ ì œì–´)
        semaphore = asyncio.Semaphore(CONCURRENCY)
        
        # 6. ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        async with httpx.AsyncClient(timeout=float(TIMEOUT)) as client:
            for chunk_idx in range(0, total_businesses, CHUNK_SIZE):
                chunk = businesses[chunk_idx:chunk_idx + CHUNK_SIZE]
                chunk_num = chunk_idx // CHUNK_SIZE + 1
                chunk_start = time.time()
                
                logger.info(f"  ðŸ“¦ ì²­í¬ {chunk_num}/{total_chunks}: {len(chunk)}ê°œ ìŒì‹ì  ì²˜ë¦¬ ì¤‘...")
                
                # ë³‘ë ¬ ì‹¤í–‰ (asyncio.gather)
                tasks = [
                    predict_for_business(client, semaphore, user_data, business, calculated_at)
                    for business in chunk
                ]
                results = await asyncio.gather(*tasks)
                
                # ê²°ê³¼ ì²˜ë¦¬ ë° DB ì €ìž¥
                chunk_success = 0
                chunk_errors = 0
                chunk_timeouts = 0
                
                for result in results:
                    if result["success"]:
                        # DBì— ì €ìž¥ (UPSERT)
                        existing = db.query(models.UserBusinessPrediction).filter(
                            and_(
                                models.UserBusinessPrediction.user_id == user_id,
                                models.UserBusinessPrediction.business_id == result["business_id"]
                            )
                        ).first()
                        
                        if existing:
                            # ì—…ë°ì´íŠ¸
                            existing.deepfm_score = result["deepfm_score"]
                            existing.multitower_score = result["multitower_score"]
                            existing.is_stale = False
                            existing.calculated_at = calculated_at
                        else:
                            # ì‹ ê·œ ì‚½ìž…
                            prediction = models.UserBusinessPrediction(
                                user_id=user_id,
                                business_id=result["business_id"],
                                deepfm_score=result["deepfm_score"],
                                multitower_score=result["multitower_score"],
                                is_stale=False,
                                calculated_at=calculated_at
                            )
                            db.add(prediction)
                        
                        chunk_success += 1
                        api_call_times.append(result["request_time"])
                        
                        # ëŠë¦° ìš”ì²­ ì¶”ì  (3ì´ˆ ì´ìƒ)
                        if result["request_time"] > 3.0:
                            slow_businesses.append({
                                "id": result["business_id"],
                                "name": result["business_name"],
                                "time": result["request_time"]
                            })
                    else:
                        # ì—ëŸ¬ ì²˜ë¦¬
                        chunk_errors += 1
                        if result.get("error") == "Timeout":
                            chunk_timeouts += 1
                            logger.error(f"     â±ï¸  íƒ€ìž„ì•„ì›ƒ: {result['business_name']} ({result['request_time']:.1f}s)")
                        else:
                            logger.warning(f"     âš ï¸  ì‹¤íŒ¨: {result['business_name']} - {result.get('error', 'Unknown')}")
                
                # ì²­í¬ë§ˆë‹¤ ì»¤ë°‹
                db.commit()
                chunk_time = time.time() - chunk_start
                
                # ì²­í¬ í†µê³„ ì—…ë°ì´íŠ¸
                success_count += chunk_success
                error_count += chunk_errors
                timeout_count += chunk_timeouts
                
                logger.info(f"  âœ… ì²­í¬ {chunk_num}/{total_chunks} ì™„ë£Œ: {chunk_time:.1f}s (ì„±ê³µ={chunk_success}, ì‹¤íŒ¨={chunk_errors})")
        
        # 7. ìµœì¢… í†µê³„ ê³„ì‚°
        total_time = time.time() - total_start_time
        
        # API í˜¸ì¶œ ì‹œê°„ í†µê³„
        if api_call_times:
            avg_time = sum(api_call_times) / len(api_call_times)
            min_time = min(api_call_times)
            max_time = max(api_call_times)
        else:
            avg_time = min_time = max_time = 0
        
        # ìµœì¢… ë¡œê·¸
        logger.info(f"âœ… [Prediction Cache] ì™„ë£Œ - ì´ ì†Œìš”ì‹œê°„: {total_time:.2f}s")
        logger.info(f"   ðŸ“Š ê²°ê³¼: ì„±ê³µ {success_count}, ì‹¤íŒ¨ {error_count} (íƒ€ìž„ì•„ì›ƒ: {timeout_count})")
        logger.info(f"   âš¡ ë³‘ë ¬ ì²˜ë¦¬: concurrency={CONCURRENCY}, ì²­í¬={total_chunks}ê°œ")
        logger.info(f"   â±ï¸  API í˜¸ì¶œ ì‹œê°„: í‰ê·  {avg_time:.2f}s, ìµœì†Œ {min_time:.2f}s, ìµœëŒ€ {max_time:.2f}s")
        
        # ëŠë¦° ìš”ì²­ë“¤ ë¡œê·¸
        if slow_businesses:
            logger.warning(f"   âš ï¸  ëŠë¦° ìš”ì²­ (3ì´ˆ ì´ìƒ): {len(slow_businesses)}ê°œ")
            # ê°€ìž¥ ëŠë¦° ê²ƒë¶€í„° ì •ë ¬
            slow_businesses.sort(key=lambda x: x["time"], reverse=True)
            for slow in slow_businesses[:5]:  # ìƒìœ„ 5ê°œë§Œ í‘œì‹œ
                logger.warning(f"      - {slow['name']} (ID: {slow['id']}): {slow['time']:.2f}s")
    
    except Exception as e:
        total_time = time.time() - total_start_time
        logger.error(f"âŒ [Prediction Cache] ì‚¬ìš©ìž {user_id} ì˜ˆì¸¡ ê³„ì‚° ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜ (ì†Œìš”: {total_time:.2f}s): {e}")
        import traceback
        traceback.print_exc()
        db.rollback()


def mark_predictions_stale(user_id: int, db: Session):
    """
    ì‚¬ìš©ìžì˜ ëª¨ë“  ì˜ˆì¸¡ì„ ìž¬ê³„ì‚° í•„ìš” ìƒíƒœë¡œ í‘œì‹œ
    
    Args:
        user_id: ì‚¬ìš©ìž ID
        db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
    """
    logger.info(f"[Prediction Cache] ì‚¬ìš©ìž {user_id}ì˜ ì˜ˆì¸¡ì„ staleë¡œ í‘œì‹œ")
    
    try:
        updated_count = db.query(models.UserBusinessPrediction).filter(
            models.UserBusinessPrediction.user_id == user_id
        ).update({"is_stale": True})
        
        db.commit()
        logger.info(f"[Prediction Cache] {updated_count}ê°œ ì˜ˆì¸¡ì„ staleë¡œ í‘œì‹œ ì™„ë£Œ")
    
    except Exception as e:
        logger.error(f"[Prediction Cache] stale í‘œì‹œ ì¤‘ ì˜¤ë¥˜: {e}")
        db.rollback()


def get_cached_predictions(user_id: int, business_ids: list, db: Session) -> dict:
    """
    ìºì‹œëœ ì˜ˆì¸¡ê°’ì„ ì¡°íšŒ
    
    Args:
        user_id: ì‚¬ìš©ìž ID
        business_ids: ì¡°íšŒí•  ìŒì‹ì  ID ë¦¬ìŠ¤íŠ¸
        db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
    
    Returns:
        {business_id: {"deepfm": score, "multitower": score, "is_stale": bool}}
    """
    predictions = db.query(models.UserBusinessPrediction).filter(
        and_(
            models.UserBusinessPrediction.user_id == user_id,
            models.UserBusinessPrediction.business_id.in_(business_ids)
        )
    ).all()
    
    result = {}
    for pred in predictions:
        result[pred.business_id] = {
            "deepfm": pred.deepfm_score,
            "multitower": pred.multitower_score,
            "is_stale": pred.is_stale
        }
    
    return result


def check_predictions_exist(user_id: int, db: Session) -> bool:
    """
    ì‚¬ìš©ìžì˜ ì˜ˆì¸¡ê°’ì´ ì¡´ìž¬í•˜ëŠ”ì§€ í™•ì¸
    
    Args:
        user_id: ì‚¬ìš©ìž ID
        db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
    
    Returns:
        bool: ì˜ˆì¸¡ê°’ ì¡´ìž¬ ì—¬ë¶€
    """
    count = db.query(models.UserBusinessPrediction).filter(
        models.UserBusinessPrediction.user_id == user_id
    ).count()
    
    return count > 0

